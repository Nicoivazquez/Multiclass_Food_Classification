{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!pip install scipy\n",
    "import tensorflow as tf\n",
    "import matplotlib.image as img\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "#from tensorflow.keras.preprocessing import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import scipy\n",
    "#Image flow from directory, data image gen\n",
    "#from skimage import io, color, filters\n",
    "#from skimage.transform import resize, rotate\n",
    "print(tf.__version__)\n",
    "print(tf.test.gpu_device_name())\n",
    "#!pip install tensorflow_datasets\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_all= '''apple_pie\n",
    "baby_back_ribs\n",
    "baklava\n",
    "beef_carpaccio\n",
    "beef_tartare\n",
    "beet_salad\n",
    "beignets\n",
    "bibimbap\n",
    "bread_pudding\n",
    "breakfast_burrito\n",
    "bruschetta\n",
    "caesar_salad\n",
    "cannoli\n",
    "caprese_salad\n",
    "carrot_cake\n",
    "ceviche\n",
    "cheesecake\n",
    "cheese_plate\n",
    "chicken_curry\n",
    "chicken_quesadilla\n",
    "chicken_wings\n",
    "chocolate_cake\n",
    "chocolate_mousse\n",
    "churros\n",
    "clam_chowder\n",
    "club_sandwich\n",
    "crab_cakes\n",
    "creme_brulee\n",
    "croque_madame\n",
    "cup_cakes\n",
    "deviled_eggs\n",
    "donuts\n",
    "dumplings\n",
    "edamame\n",
    "eggs_benedict\n",
    "escargots\n",
    "falafel\n",
    "filet_mignon\n",
    "fish_and_chips\n",
    "foie_gras\n",
    "french_fries\n",
    "french_onion_soup\n",
    "french_toast\n",
    "fried_calamari\n",
    "fried_rice\n",
    "frozen_yogurt\n",
    "garlic_bread\n",
    "gnocchi\n",
    "greek_salad\n",
    "grilled_cheese_sandwich\n",
    "grilled_salmon\n",
    "guacamole\n",
    "gyoza\n",
    "hamburger\n",
    "hot_and_sour_soup\n",
    "hot_dog\n",
    "huevos_rancheros\n",
    "hummus\n",
    "ice_cream\n",
    "lasagna\n",
    "lobster_bisque\n",
    "lobster_roll_sandwich\n",
    "macaroni_and_cheese\n",
    "macarons\n",
    "miso_soup\n",
    "mussels\n",
    "nachos\n",
    "omelette\n",
    "onion_rings\n",
    "oysters\n",
    "pad_thai\n",
    "paella\n",
    "pancakes\n",
    "panna_cotta\n",
    "peking_duck\n",
    "pho\n",
    "pizza\n",
    "pork_chop\n",
    "poutine\n",
    "prime_rib\n",
    "pulled_pork_sandwich\n",
    "ramen\n",
    "ravioli\n",
    "red_velvet_cake\n",
    "risotto\n",
    "samosa\n",
    "sashimi\n",
    "scallops\n",
    "seaweed_salad\n",
    "shrimp_and_grits\n",
    "spaghetti_bolognese\n",
    "spaghetti_carbonara\n",
    "spring_rolls\n",
    "steak\n",
    "strawberry_shortcake\n",
    "sushi\n",
    "tacos\n",
    "takoyaki\n",
    "tiramisu\n",
    "tuna_tartare\n",
    "waffles'''\n",
    "user_words = labels_all\n",
    "word_list = user_words.split()\n",
    "user_words = []\n",
    "for word in word_list:\n",
    "    user_words.append(word)\n",
    "user_words = \" \".join(user_words)\n",
    "class_labels = user_words.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making only 5 classes\n",
    "\n",
    "# possible agumentations (rotation_range=90, rescale=1./255,rotation_range=90,height_shift_range=0.5,shear_range=0.1, zoom_range=0.1, horizontal_flip=True))\n",
    "\n",
    "X5_train_datagen = ImageDataGenerator(rescale=1./255,horizontal_flip=True,shear_range=0.2, zoom_range=0.2)\n",
    "X5_test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "X5_var_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "labels_5 = ['apple_pie','waffles','gnocchi','chocolate_mousse','baklava']\n",
    "\n",
    "X5_train = X5_train_datagen.flow_from_directory(\n",
    "    '../data/train_mini/',\n",
    "    target_size=(256,256),\n",
    "    color_mode=\"rgb\",\n",
    "    classes=labels_5,\n",
    "    class_mode=\"categorical\",\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    seed=None,\n",
    "    save_to_dir=None,\n",
    "    save_prefix=\"\",\n",
    "    save_format=\"png\",\n",
    "    follow_links=False,\n",
    "    subset=None,\n",
    "    interpolation=\"nearest\",\n",
    ")\n",
    "\n",
    "X5_test = X5_test_datagen.flow_from_directory(\n",
    "    '../data/test_mini/',\n",
    "    target_size=(256,256),\n",
    "    color_mode=\"rgb\",\n",
    "    classes=labels_5,\n",
    "    class_mode=\"categorical\",\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    seed=None,\n",
    "    save_to_dir=None,\n",
    "    save_prefix=\"\",\n",
    "    save_format=\"png\",\n",
    "    follow_links=False,\n",
    "    subset=None,\n",
    "    interpolation=\"nearest\",\n",
    ")\n",
    "X5_var = X5_var_datagen.flow_from_directory(\n",
    "    '../data/var_mini/',\n",
    "    target_size=(256,256),\n",
    "    color_mode=\"rgb\",\n",
    "    classes=labels_5,\n",
    "    class_mode=\"categorical\",\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    seed=None,\n",
    "    save_to_dir=None,\n",
    "    save_prefix=\"\",\n",
    "    save_format=\"png\",\n",
    "    follow_links=False,\n",
    "    subset=None,\n",
    "    interpolation=\"nearest\",\n",
    ")\n",
    "\n",
    "print('These waffles have been: height_shift_range=0.5,shear_range=0.2, zoom_range=0.2, horizontal_flip=True')\n",
    "from numpy import expand_dims\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from matplotlib import pyplot\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 10, 10\n",
    "# load the image\n",
    "img = load_img('../data/train_mini/waffles/1005755.jpg')\n",
    "# convert to numpy array\n",
    "data = img_to_array(img)\n",
    "# expand dimension to one sample\n",
    "samples = expand_dims(data, 0)\n",
    "# create image data augmentation generator\n",
    "#datagen = ImageDataGenerator(height_shift_range=0.5)\n",
    "# prepare iterator\n",
    "it = X5_train_datagen.flow(samples, batch_size=1)\n",
    "# generate samples and plot\n",
    "pyplot.title(label='These waffles have bee height_shift_range=0.5,shear_range=0.2, zoom_range=0.2, horizontal_flip=True')\n",
    "for i in range(9):\n",
    "    # define subplot\n",
    "    pyplot.subplot( 330 +1 +i)\n",
    "    # generate batch of images\n",
    "    batch = it.next()\n",
    "    # convert to unsigned integers for viewing\n",
    "    image = batch[0].astype('uint8')\n",
    "    # plot raw pixel data\n",
    "    pyplot.minorticks_off()\n",
    "    pyplot.imshow(image)\n",
    "    pyplot.axis('off')\n",
    "    \n",
    "# show the figure, works only when rescale 1/255 is off. \n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting logs file ready for tensorboard\n",
    "root_logdir = os.path.join(os.curdir, \"my_logs\")\n",
    "def get_run_logdir():\n",
    "    import time\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "    return os.path.join(root_logdir, run_id)\n",
    "run_logdir = get_run_logdir()\n",
    "\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10,restore_best_weights=True, monitor = 'val_loss') \n",
    "checkpoint_filepath = './CNN/custom_5classmodel.h5'\n",
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard dev upload --logdir \\\n",
    "    './my_logs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(kernel_size=(3,3), input_shape=[256,256,3], pool_size=2, nb_classes=5):\n",
    "    model = Sequential() \n",
    "    # options: 'linear', 'sigmoid', 'tanh', 'relu', 'softplus', 'softsign'\n",
    "    model.add(Conv2D(16, (kernel_size[0], kernel_size[1]),\n",
    "                        padding='same', \n",
    "                        input_shape=input_shape)) \n",
    "    model.add(Activation('relu')) \n",
    "    model.add(Conv2D(64, (kernel_size[0], kernel_size[1]), padding='same')) \n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=pool_size)) # decreases size, prevent overfitting\n",
    "    model.add(Flatten()) # necessary to flatten before going into conventional dense layer\n",
    "    print('Model flattened out to ', model.output_shape)\n",
    "    # now start a neural network\n",
    "    model.add(Dense(64))\n",
    "    model.add(Activation('relu')) #64 neurons in this layer\n",
    "    model.add(Dense(128))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2)) # zeros out some fraction of inputs, helps prevent overfitting\n",
    "    model.add(Dense(nb_classes)) # (one for each class)\n",
    "    model.add(Activation('softmax')) # softmax at end to pick between classes 0-3 KEEP\n",
    "    # optimizers to one of these: 'adam', 'adadelta', 'sgd'\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                optimizer='adam',\n",
    "                metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model_complex = define_model()\n",
    "\n",
    "\n",
    "\n",
    "history = model_complex.fit(\n",
    "        X5_train,\n",
    "        steps_per_epoch=3500//100,\n",
    "        epochs=50,\n",
    "        validation_data=X5_var,\n",
    "        validation_steps=750//100)\n",
    "#callbacks=[checkpoint_cb,early_stopping_cb,tensorboard_cb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "score = model.evaluate(X5_var, verbose=0)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1]) # this is the one we care about"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(50)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X5_test.reset()\n",
    "predIdxs = history.predict(x=X5_test,\n",
    "    steps=(750 // 32) + 1)\n",
    "predIdxs = np.argmax(predIdxs, axis=1)\n",
    "print(classification_report(X5_test.classes, predIdxs,\n",
    "    target_names=X5_test.class_indices.keys()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading saved model \n",
    "name = keras.models.load_model('./CNN/custom_5classmodel.h5')\n",
    "predictions_ = name.predict(X5_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "X5_test.reset()\n",
    "predIdxs = modelw.predict(x=X5_test,\n",
    "\tsteps=(750 // 32) + 1)\n",
    "predIdxs = np.argmax(predIdxs, axis=1)\n",
    "print(classification_report(X5_test.classes, predIdxs,\n",
    "\ttarget_names=X5_test.class_indices.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model2(kernel_size=(3,3), input_shape=[256,256,3], pool_size=3, nb_classes=5):\n",
    "    model = Sequential() \n",
    "    model.add(Conv2D(16, (kernel_size[0], kernel_size[1]),\n",
    "                        padding='same', \n",
    "                        input_shape=input_shape)) \n",
    "    model.add(Activation('relu')) \n",
    "    model.add(Conv2D(32, (kernel_size[0], kernel_size[1]), padding='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=pool_size)) \n",
    "    model.add(Conv2D(64, (kernel_size[0], kernel_size[1]),\n",
    "                        padding='same', \n",
    "                        input_shape=input_shape))\n",
    "    model.add(Flatten()) # necessary to flatten before going into conventional dense layer  KEEP\n",
    "    print('Model flattened out to ', model.output_shape)\n",
    "    # now start a typical neural network\n",
    "    model.add(Dense(128)) # (only) 32 neurons in this layer, really?   KEEP\n",
    "    model.add(Activation('relu'))\n",
    "#     model.add(Dropout(0.1)) # zeros out some fraction of inputs, helps prevent overfitting\n",
    "    model.add(Dense(nb_classes)) # 5 final nodes (one for each class)  KEEP\n",
    "    model.add(Activation('softmax')) # softmax at end to pick between classes 0-3 KEEP\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                optimizer='adam',\n",
    "                metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model2 = define_model2()\n",
    "\n",
    "#384,384\n",
    "\n",
    "history2 = model2.fit(\n",
    "        X5_train,\n",
    "        steps_per_epoch=3500//32,\n",
    "        epochs=15,\n",
    "        validation_data=X5_var,\n",
    "        validation_steps=750//32,\n",
    "        callbacks=[checkpoint_cb,early_stopping_cb,tensorboard_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model3(kernel_size=(3,3), input_shape=[256,256,3], pool_size=4, nb_classes=5):\n",
    "    model = Sequential() \n",
    "    # options: 'linear', 'sigmoid', 'tanh', 'relu', 'softplus', 'softsign'\n",
    "    model.add(Conv2D(16, (kernel_size[0], kernel_size[1]),\n",
    "                        padding='same', strides=1,\n",
    "                        input_shape=input_shape)) \n",
    "    model.add(Activation('relu')) # Activation specification necessary for Conv2D and Dense layers\n",
    "    model.add(Conv2D(16, (kernel_size[0], kernel_size[1]), padding='same')) \n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=pool_size)) # decreases size, helps prevent overfitting\n",
    "    model.add(Conv2D(64, (kernel_size[0], kernel_size[1]),\n",
    "                        padding='same', strides=1,\n",
    "                        input_shape=input_shape))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(64, (kernel_size[0], kernel_size[1]),\n",
    "                        padding='same', strides=1,\n",
    "                        input_shape=input_shape))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Flatten()) # necessary to flatten before going into conventional dense layer\n",
    "    print('Model flattened out to ', model.output_shape)\n",
    "    # now start a typical neural network\n",
    "    model.add(Dense(64))   \n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(nb_classes)) # 5 final nodes (one for each class) \n",
    "    model.add(Activation('softmax')) \n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                optimizer='adam',\n",
    "                metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model3 = define_model3()\n",
    "\n",
    "\n",
    "history3 = model3.fit(\n",
    "        X5_train,\n",
    "        steps_per_epoch=3500//32,\n",
    "        epochs=50,\n",
    "        validation_data=X5_var,\n",
    "        validation_steps=750//32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"accuracy\"])\n",
    "plt.plot(history.history[\"val_accuracy\"])\n",
    "plt.plot(history.history[\"loss\"])\n",
    "plt.plot(history.history[\"val_loss\"])\n",
    "plt.title(\"model accuracy\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend([\"Training Accuracy\",\"Validation Accuracy\",\"Training loss\",\"Validation Loss\"])\n",
    "plt.show()\n",
    "\n",
    " \n",
    "X5_test.reset()\n",
    "predIdxs = model_on_.predict(x=X5_test,\n",
    "    steps=(750 // 32) + 1)\n",
    "predIdxs = np.argmax(predIdxs, axis=1)\n",
    "print(classification_report(X5_test.classes, predIdxs,\n",
    "    target_names=X5_test.class_indices.keys()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ran out of power must SSH into some power on AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
